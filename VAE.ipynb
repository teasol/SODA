{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy import io\n",
    "from scipy import sparse\n",
    "import scipy\n",
    "import gzip\n",
    "import scanpy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, mtx):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.mtx = mtx.tocsc()\n",
    "        self.number_of_genes, self.number_of_cells = mtx.shape\n",
    "    def __len__(self):\n",
    "        return self.number_of_cells\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = torch.FloatTensor(np.asarray(self.mtx[:, idx].todense()).squeeze())        \n",
    "        return X\n",
    "\n",
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, number_of_genes):\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.number_of_genes = number_of_genes\n",
    "\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Linear(self.number_of_genes, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.linear_a = nn.Linear(100, 10)\n",
    "        self.linear_b = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "        self.decode = nn.Linear(10, self.number_of_genes)\n",
    "        self.decode.weight.data.fill_(0.5)\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        x = self.encode(x)\n",
    "        a = torch.exp(self.linear_a(x))\n",
    "        b = torch.exp(self.linear_b(x))\n",
    "\n",
    "        # Random sampling (reparametrization trick)\n",
    "        z = Beta(a, b).sample()\n",
    "\n",
    "        # Decoding\n",
    "        x_decoded = self.decode(z)\n",
    "\n",
    "        \n",
    "        return x_decoded\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, number_of_genes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.number_of_genes = number_of_genes\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.number_of_genes, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        z = torch.sigmoid(x)\n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp Data\n",
    "heart = normalize(heart_raw)\n",
    "number_of_genes = heart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    mse = nn.MSELoss()\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, X in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred = model.forward(X)\n",
    "        loss = mse(pred, X)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        model.decode.weight.data.copy_(torch.sigmoid(model.decode.weight.data))\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch', epoch)\n",
    "        print('TRAIN loss =', average_loss*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(heart)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "device = 'cuda:0'\n",
    "vae = BetaVAE(number_of_genes).to(device)\n",
    "learning_rate = 0.000002\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train(vae, device, train_loader, optimizer, epoch)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, device, train_loader, vae, discriminator, gen_opt, disc_opt):\n",
    "    vae.train()\n",
    "    discriminator.train()\n",
    "    total_g = 0\n",
    "    total_d = 0\n",
    "\n",
    "    bce = nn.BCELoss()\n",
    "\n",
    "    for X in train_loader:\n",
    "        X = X.to(device)\n",
    "\n",
    "        size = discriminator(X).size()\n",
    "        ones = torch.ones(size, requires_grad=False).to(device)\n",
    "        zeros = torch.zeros(size, requires_grad=False).to(device)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        X_ = vae(X)\n",
    "        gen_loss = bce(discriminator(X_), ones)\n",
    "        gen_loss.backward()\n",
    "        total_g += gen_loss.item()\n",
    "        gen_opt.step()\n",
    "\n",
    "        disc_opt.zero_grad()        \n",
    "        actual_loss = bce(discriminator(X), ones)\n",
    "        fake_loss = bce(discriminator(X_.detach()), zeros)\n",
    "        disc_loss = (actual_loss + fake_loss) / 2\n",
    "        disc_loss.backward()\n",
    "        total_d += disc_loss.item()\n",
    "        disc_opt.step()\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch', epoch, 'G', total_g/len(train_loader), 'D', total_d/len(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immune_dataset = CustomDataset(immune)\n",
    "dataset = immune_dataset\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "device = 'cuda:4'\n",
    "learning_rate = 0.01\n",
    "epochs = 100\n",
    "\n",
    "vae = BetaVAE(number_of_genes)\n",
    "vae.load_state_dict(torch.load('/data/home/kimds/GAN/betavae.pt'))\n",
    "discriminator = Discriminator(number_of_genes)\n",
    "gen_opt = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "disc_opt = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch, device, train_loader, vae, discriminator, gen_opt, disc_opt)\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('TORCH': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d12e7c042276a4aee0aba75551736f84218c16c7b0909dd9737c86aed213d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
